{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lorenchamplin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lorenchamplin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lorenchamplin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lorenchamplin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "from sklearn import metrics\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from dateutil import parser\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import sklearn.cluster as sc\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "s=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(input):\n",
    "  \"\"\"\n",
    "  Returns lowercase text\n",
    "  \"\"\"\n",
    "  return input.lower()\n",
    "\n",
    "def remove_punctuation(input):\n",
    "  \"\"\"\n",
    "  Returns text without punctuation\n",
    "  \"\"\"\n",
    "  return input.translate(str.maketrans('','', string.punctuation+'–’”“—·'))\n",
    "\n",
    "def remove_whitespaces(input):\n",
    "  \"\"\"\n",
    "  Returns text without extra whitespaces\n",
    "  \"\"\"\n",
    "  return \" \".join(input.split())\n",
    "  \n",
    "def remove_html_tags(input):\n",
    "  \"\"\"\n",
    "  Returns text without HTML tags\n",
    "  \"\"\"\n",
    "  soup = BeautifulSoup(input, \"html.parser\")\n",
    "  stripped_input = soup.get_text(separator=\" \")\n",
    "  return stripped_input\n",
    "\n",
    "def tokenize(input):\n",
    "  \"\"\"\n",
    "  Returns tokenized version of text\n",
    "  \"\"\"\n",
    "  return word_tokenize(input)\n",
    "\n",
    "def remove_stop_words(input):\n",
    "  \"\"\"\n",
    "  Returns text without stop words\n",
    "  \"\"\"\n",
    "  input = word_tokenize(input)\n",
    "  return [word for word in input if word not in stopwords.words('english') or word == \"no\" or word == \"not\"]\n",
    "\n",
    "def lemmatize(input):\n",
    "  \"\"\"\n",
    "  Lemmatizes input using NLTK's WordNetLemmatizer\n",
    "  \"\"\"\n",
    "  lemmatizer=WordNetLemmatizer()\n",
    "  input_str=word_tokenize(input)\n",
    "  new_words = []\n",
    "  for word in input_str:\n",
    "    new_words.append(lemmatizer.lemmatize(word))\n",
    "  return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def nlp_pipeline(input):\n",
    "  \"\"\"\n",
    "  Function that calls all other functions together to perform NLP on a given text\n",
    "  \"\"\"\n",
    "  return lemmatize(' '.join(remove_stop_words(remove_whitespaces(remove_punctuation(remove_html_tags(lowercase(input)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = tp.LDAModel(tw=tp.TermWeight.ONE,k=6,seed=2021)\n",
    "data = []\n",
    "sentences = []\n",
    "truth_labels = []\n",
    "with open(\"../comments.csv\", newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        truth_labels.append(row[1])\n",
    "        sentences.append(row[0])\n",
    "        line = nlp_pipeline(row[0]).split()\n",
    "        if line:\n",
    "            mdl.add_doc(nlp_pipeline(row[0]).split())\n",
    "            data.append(nlp_pipeline(row[0]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Basic Info>\n",
      "| LDAModel (current version: 0.12.2)\n",
      "| 694 docs, 21405 words\n",
      "| Total Vocabs: 3117, Used Vocabs: 3117\n",
      "| Entropy of words: 6.48915\n",
      "| Entropy of term-weighted words: 6.48915\n",
      "| Removed Vocabs: <NA>\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 500, Burn-in steps: 0\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -6.96409\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.ONE\n",
      "| min_cf: 0 (minimum collection frequency of words)\n",
      "| min_df: 0 (minimum document frequency of words)\n",
      "| rm_top: 0 (the number of top words to be removed)\n",
      "| k: 6 (the number of topics between 1 ~ 32767)\n",
      "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| seed: 2021 (random seed)\n",
      "| trained in version 0.12.2\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (Dirichlet prior on the per-document topic distributions)\n",
      "|  [0.2959824  0.20503452 0.46033734 0.9188642  0.17112832 0.11902425]\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "|\n",
      "<Topics>\n",
      "| #0 (3029) : arizona state political group not\n",
      "| #1 (2636) : national company look say inappropriate\n",
      "| #2 (4535) : firm independent haystaqdna party partisan\n",
      "| #3 (6535) : redistricting company haystaq not no\n",
      "| #4 (2332) : public comment not firm would\n",
      "| #5 (2338) : map district ndc mapping community\n",
      "|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 500, 10):\n",
    "    mdl.train(10)\n",
    "\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in data:\n",
    "    samples.append(mdl.make_doc(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mdl.infer(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_evaluate(texts, X,truth_labels):\n",
    "    clustering = sc.AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=0.1,\n",
    "        affinity='cosine',\n",
    "        linkage='average')\n",
    "    clustering.fit(X)\n",
    "    print(\"\\n\")\n",
    "    print(\"=== Unsupervised Metrics ===\\n\")\n",
    "    print(\"== Silhouette Coefficient [0,1*]==\")\n",
    "    print(metrics.silhouette_score(X, clustering.labels_, metric='cosine'),\"\\n\")\n",
    "    print(\"== Calinski-Harabasz Index [0,inf*] ==\")\n",
    "    print(metrics.calinski_harabasz_score(X, clustering.labels_),\"\\n\")\n",
    "    print(\"== Davies-Bouldin Index [0*,1] ==\")\n",
    "    print(metrics.davies_bouldin_score(X, clustering.labels_),\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(\"=== Supervised Metrics ===\\n\")\n",
    "    print(\"== Rand Index [0,1*] ==\")\n",
    "    print(metrics.rand_score(truth_labels, clustering.labels_),\"\\n\")\n",
    "    print(\"== Normalized Mutual Information Score [0,1*] ==\")\n",
    "    print(metrics.normalized_mutual_info_score(truth_labels, clustering.labels_),\"\\n\")\n",
    "    print(\"== Fowlkes-Mallows Score [0,1*] ==\")\n",
    "    print(metrics.fowlkes_mallows_score(truth_labels, clustering.labels_),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Unsupervised Metrics ===\n",
      "\n",
      "== Silhouette Coefficient [0,1*]==\n",
      "0.51889884 \n",
      "\n",
      "== Calinski-Harabasz Index [0,inf*] ==\n",
      "158.85115380748968 \n",
      "\n",
      "== Davies-Bouldin Index [0*,1] ==\n",
      "0.8832133511288746 \n",
      "\n",
      "\n",
      "\n",
      "=== Supervised Metrics ===\n",
      "\n",
      "== Rand Index [0,1*] ==\n",
      "0.5372123873564796 \n",
      "\n",
      "== Normalized Mutual Information Score [0,1*] ==\n",
      "0.26700745506587076 \n",
      "\n",
      "== Fowlkes-Mallows Score [0,1*] ==\n",
      "0.4048308619059552 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_and_evaluate(sentences, X,truth_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
